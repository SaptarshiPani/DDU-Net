{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10316661,"sourceType":"datasetVersion","datasetId":6118595},{"sourceId":10357553,"sourceType":"datasetVersion","datasetId":6414413},{"sourceId":6107556,"sourceType":"datasetVersion","datasetId":3498826}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.distributions as dist\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nimport torchvision.transforms as transforms\nimport torchvision\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport os\nfrom PIL import Image\nimport seaborn as sns\n\nclass DSTFusion(nn.Module):\n    \"\"\"Dempster-Shafer Theory based feature fusion module\"\"\"\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.conv_mass1 = nn.Conv2d(in_channels, 3, kernel_size=1)  # m_fg, m_bg, m_unc\n        self.conv_mass2 = nn.Conv2d(in_channels, 3, kernel_size=1)\n        self.combined_conv = nn.Conv2d(3, out_channels, kernel_size=1)\n        self.softmax = nn.Softmax(dim=1)\n\n    def dempster_combine(self, m1, m2):\n        # m1, m2: (batch, 3, H, W)\n        m1 = self.softmax(m1)\n        m2 = self.softmax(m2)\n        \n        m1_fg, m1_bg, m1_unc = m1[:,0], m1[:,1], m1[:,2]\n        m2_fg, m2_bg, m2_unc = m2[:,0], m2[:,1], m2[:,2]\n        \n        # Calculate conflict\n        conflict = m1_fg*m2_bg + m1_bg*m2_fg + 1e-8\n        \n        # Combine masses\n        m_fg = (m1_fg*m2_fg + m1_fg*m2_unc + m2_fg*m1_unc) / (1 - conflict)\n        m_bg = (m1_bg*m2_bg + m1_bg*m2_unc + m2_bg*m1_unc) / (1 - conflict)\n        m_unc = (m1_unc*m2_unc) / (1 - conflict)\n        \n        # Normalize\n        sum_m = m_fg + m_bg + m_unc\n        combined = torch.stack([m_fg/sum_m, m_bg/sum_m, m_unc/sum_m], dim=1)\n        return combined\n\n    def forward(self, x1, x2):\n        # x1: upsampled features, x2: encoder features\n        m1 = self.conv_mass1(x1)\n        m2 = self.conv_mass2(x2)\n        combined_mass = self.dempster_combine(m1, m2)\n        return self.combined_conv(combined_mass)\n\nclass UNet(nn.Module):\n    def __init__(self, input_channels=4, output_channels=1):\n        super(UNet, self).__init__()\n        \n        # Encoder\n        self.enc1 = self.conv_block(input_channels, 64)\n        self.enc2 = self.conv_block(64, 128)\n        self.enc3 = self.conv_block(128, 256)\n        self.enc4 = self.conv_block(256, 512)\n        \n        # Bottleneck\n        self.bottleneck = self.conv_block(512, 1024)\n        \n        # Decoder with DST Fusion\n        self.up4 = self.upconv(1024, 512)\n        self.dst_fusion4 = DSTFusion(512, 512)\n        self.dec4 = self.conv_block(512, 512)\n        \n        self.up3 = self.upconv(512, 256)\n        self.dst_fusion3 = DSTFusion(256, 256)\n        self.dec3 = self.conv_block(256, 256)\n        \n        self.up2 = self.upconv(256, 128)\n        self.dst_fusion2 = DSTFusion(128, 128)\n        self.dec2 = self.conv_block(128, 128)\n        \n        self.up1 = self.upconv(128, 64)\n        self.dst_fusion1 = DSTFusion(64, 64)\n        self.dec1 = self.conv_block(64, 64)\n        \n        # Output layers\n        self.final = nn.Conv2d(64, 3, kernel_size=3, padding=1)\n        self.final2 = nn.Conv2d(3, output_channels, kernel_size=1)\n        self.out_act = nn.Sigmoid()\n        \n    def conv_block(self, in_channels, out_channels):\n        return nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(),\n            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(),\n        )\n    \n    def upconv(self, in_channels, out_channels):\n        return nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n    \n    def forward(self, x, condition):\n        # Concatenate condition\n        y = torch.cat((x, condition), dim=1)\n        \n        # Encoder\n        enc1 = self.enc1(y)\n        enc2 = self.enc2(F.max_pool2d(enc1, 2))\n        enc3 = self.enc3(F.max_pool2d(enc2, 2))\n        enc4 = self.enc4(F.max_pool2d(enc3, 2))\n        \n        # Bottleneck\n        bottleneck = self.bottleneck(F.max_pool2d(enc4, 2))\n        \n        # Decoder with DST Fusion\n        up4 = self.up4(bottleneck)\n        fuse4 = self.dst_fusion4(up4, enc4)\n        dec4 = self.dec4(fuse4)\n        \n        up3 = self.up3(dec4)\n        fuse3 = self.dst_fusion3(up3, enc3)\n        dec3 = self.dec3(fuse3)\n        \n        up2 = self.up2(dec3)\n        fuse2 = self.dst_fusion2(up2, enc2)\n        dec2 = self.dec2(fuse2)\n        \n        up1 = self.up1(dec2)\n        fuse1 = self.dst_fusion1(up1, enc1)\n        dec1 = self.dec1(fuse1)\n        \n        # Residual connection with uncertainty-aware output\n        out = self.out_act(self.final2(x - self.final(dec1)))\n        return out\n\nclass Diffusion:\n    def __init__(self, T=100, beta_start=1e-4, beta_end=0.02, device=False):\n        self.device = device\n        self.T = T\n        self.betas = torch.linspace(beta_start, beta_end, T)\n        self.alphas = 1.0 - self.betas\n        self.alpha_hat = torch.cumprod(self.alphas, dim=0).to(device)\n    \n    def q_sample(self, x_start, t, noise=None):\n        if noise is None:\n            noise = torch.randn_like(x_start)\n        sqrt_alpha_hat = torch.sqrt(self.alpha_hat[t])[:, None, None, None]\n        sqrt_one_minus_alpha_hat = torch.sqrt(1 - self.alpha_hat[t])[:, None, None, None]\n        return sqrt_alpha_hat * x_start + sqrt_one_minus_alpha_hat * noise, noise\n\nclass SegmentationDataset(Dataset):\n    def __init__(self, image_dir, mask_dir, transform=None, target_transform=None):\n        self.image_dir = image_dir\n        self.mask_dir = mask_dir\n        self.image_filenames = sorted(os.listdir(image_dir))\n        self.mask_filenames = sorted(os.listdir(mask_dir))\n        self.transform = transform\n        self.target_transform = target_transform\n\n    def __len__(self):\n        return len(self.image_filenames)\n\n    def __getitem__(self, idx):\n        image_path = os.path.join(self.image_dir, self.image_filenames[idx])\n        mask_path = os.path.join(self.mask_dir, self.mask_filenames[idx])\n        \n        image = Image.open(image_path).convert(\"RGB\")\n        mask = Image.open(mask_path).convert(\"L\")\n        \n        if self.transform:\n            image = self.transform(image)\n        if self.target_transform:\n            mask = self.target_transform(mask)\n        \n        return image, mask\n\n# Define transformations\nimage_transform = transforms.Compose([\n    transforms.Resize((128, 128)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n])\n\nmask_transform = transforms.Compose([\n    transforms.Resize((128, 128)),\n    transforms.ToTensor(),\n])\n\n# Dataset paths\ntrain_image_dir = \"\"\ntrain_mask_dir = \"\"\ntest_image_dir = \"\"\ntest_mask_dir = \"\"\n\n# Create datasets\ntrain_dataset = SegmentationDataset(train_image_dir, train_mask_dir, \n                                  transform=image_transform, \n                                  target_transform=mask_transform)\ntest_dataset = SegmentationDataset(test_image_dir, test_mask_dir,\n                                 transform=image_transform,\n                                 target_transform=mask_transform)\n\n# DataLoaders\ntrain_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=2)\ntest_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, num_workers=2)\n\n# Check sample batch\nif __name__ == \"__main__\":\n    for images, masks in train_loader:\n        print(\"Image batch shape:\", images.shape)\n        print(\"Mask batch shape:\", masks.shape)\n        break\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = UNet(input_channels=4, output_channels=1).to(device)\ndiffusion = Diffusion(T=100, device=device)\n\n# Loss and optimizer\ncriterion = nn.BCELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n# Training loop\nfor epoch in range(100):\n    model.train()\n    epoch_loss = 0\n    for images, masks in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n        images, masks = images.to(device), masks.to(device, dtype=torch.float)\n\n        # Sample timestep\n        t = torch.randint(0, diffusion.T, (images.size(0),), device=device)\n        \n        # Add noise to images\n        noisy_images, noise = diffusion.q_sample(images, t)\n        \n        # Predict noise conditioned on masks\n        noise_pred = model(noisy_images, masks)\n        \n        # Compute loss\n        loss = criterion(noise_pred, masks)\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        epoch_loss += loss.item()\n\n    print(f\"Epoch {epoch+1}, Loss: {epoch_loss / len(train_loader):.4f}\")\n    \n    # Visualization\n    with torch.no_grad():\n        if epoch % 50 == 0:\n            sample_images, sample_masks = images.cpu(), masks.cpu()\n            noisy_sample_images = noisy_images.cpu()\n            generated_images = noise_pred.cpu()\n            \n            fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n            axes[0].imshow(sample_images[0].permute(1, 2, 0).numpy() * 0.5 + 0.5)\n            axes[0].set_title(\"Input Image\")\n            axes[1].imshow(noisy_sample_images[0].permute(1, 2, 0).numpy() * 0.5 + 0.5)\n            axes[1].set_title(\"Noisy Image\")\n            axes[2].imshow(generated_images[0, 0], cmap='gray')\n            axes[2].set_title(\"Predicted Mask\")\n            axes[3].imshow(sample_masks[0, 0], cmap='gray')\n            axes[3].set_title(\"Ground Truth\")\n            \n            for ax in axes:\n                ax.axis('off')\n            plt.show()\n\ndef calculate_metrics(pred, target):\n    pred = (pred > 0.5).float()\n    target = target.float()\n    \n    intersection = torch.sum(pred * target)\n    union = torch.sum(pred) + torch.sum(target) - intersection\n    dice = (2.0 * intersection) / (torch.sum(pred) + torch.sum(target) + 1e-8)\n    \n    true_positive = torch.sum(pred * target)\n    false_positive = torch.sum(pred * (1 - target))\n    false_negative = torch.sum((1 - pred) * target)\n    \n    accuracy = torch.sum(pred == target) / torch.numel(target)\n    precision = true_positive / (true_positive + false_positive + 1e-8)\n    recall = true_positive / (true_positive + false_negative + 1e-8)\n    \n    iou = intersection / (union + 1e-8)\n    \n    return iou.item(), dice.item(), accuracy.item(), precision.item(), recall.item()\n\ndef evaluate(model, dataloader, criterion, diffusion, device, visualize=False):\n    model.eval()\n    total_loss = 0\n    total_iou, total_dice = 0, 0\n    total_accuracy, total_precision, total_recall = 0, 0, 0\n    num_batches = 0\n    \n    with torch.no_grad():\n        for images, masks in tqdm(dataloader, desc=\"Evaluating\"):\n            images, masks = images.to(device), masks.to(device, dtype=torch.float)\n            \n            t = torch.randint(0, diffusion.T, (images.size(0),), device=device)\n            noisy_images, noise = diffusion.q_sample(images, t)\n            noise_pred = model(noisy_images, masks)\n            \n            loss = criterion(noise_pred, masks)\n            total_loss += loss.item()\n\n            iou, dice, accuracy, precision, recall = calculate_metrics(noise_pred, masks)\n            total_iou += iou\n            total_dice += dice\n            total_accuracy += accuracy\n            total_precision += precision\n            total_recall += recall\n            num_batches += 1\n\n            if visualize and num_batches == 1:\n                fig, axes = plt.subplots(4, 4, figsize=(12, 12))\n                for i in range(4):\n                    axes[i, 0].imshow(images[i].cpu().permute(1, 2, 0).numpy() * 0.5 + 0.5)\n                    axes[i, 1].imshow(noisy_images[i].cpu().permute(1, 2, 0).numpy() * 0.5 + 0.5)\n                    axes[i, 2].imshow(noise_pred[i, 0].cpu(), cmap='gray')\n                    axes[i, 3].imshow(masks[i, 0].cpu(), cmap='gray')\n\n                    for j in range(4):\n                        axes[i, j].axis('off')\n                plt.tight_layout()\n                plt.show()\n                \n                # Save the first test image and its prediction for heatmap generation\n                test_img = images[0].cpu().permute(1, 2, 0).numpy() * 0.5 + 0.5\n                test_mask = masks[0, 0].cpu().numpy()\n                test_pred = noise_pred[0, 0].cpu().numpy()\n\n    average_loss = total_loss / num_batches\n    average_iou = total_iou / num_batches\n    average_dice = total_dice / num_batches\n    average_accuracy = total_accuracy / num_batches\n    average_precision = total_precision / num_batches\n    average_recall = total_recall / num_batches\n\n    print(f\"\\nEvaluation Metrics:\")\n    print(f\"Average Loss: {average_loss:.4f}\")\n    print(f\"IoU: {average_iou:.4f}\")\n    print(f\"Dice: {average_dice:.4f}\")\n    print(f\"Accuracy: {average_accuracy:.4f}\")\n    print(f\"Precision: {average_precision:.4f}\")\n    print(f\"Recall: {average_recall:.4f}\")\n    \n    return {\n        \"loss\": average_loss,\n        \"iou\": average_iou,\n        \"dice\": average_dice,\n        \"accuracy\": average_accuracy,\n        \"precision\": average_precision,\n        \"recall\": average_recall,\n        \"test_img\": test_img,\n        \"test_mask\": test_mask,\n        \"test_pred\": test_pred\n    }\n\n# Final evaluation\nprint(\"\\nFinal Evaluation on Test Set:\")\ntest_metrics = evaluate(model, test_loader, criterion, diffusion, device, visualize=True)\n\n# Generate and display heatmap for a single test image\nplt.figure(figsize=(18, 6))\n\n# Original Image\nplt.subplot(1, 3, 1)\nplt.imshow(test_metrics[\"test_img\"])\nplt.title(\"Original Image\")\nplt.axis('off')\n\n# Ground Truth Mask\nplt.subplot(1, 3, 2)\nplt.imshow(test_metrics[\"test_mask\"], cmap='gray')\nplt.title(\"Ground Truth Mask\")\nplt.axis('off')\n\n# Prediction Heatmap\nplt.subplot(1, 3, 3)\nheatmap = sns.heatmap(test_metrics[\"test_pred\"], cmap='viridis', cbar=True, \n                     xticklabels=False, yticklabels=False)\nheatmap.set_title(\"Prediction Confidence Heatmap\")\nplt.tight_layout()\nplt.show()\n\n# Additional visualization: Overlay heatmap on original image\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplt.imshow(test_metrics[\"test_img\"])\nplt.imshow(test_metrics[\"test_pred\"], cmap='jet', alpha=0.5)\nplt.title(\"Prediction Heatmap Overlay\")\nplt.axis('off')\n\nplt.subplot(1, 2, 2)\nplt.imshow(test_metrics[\"test_img\"])\nplt.imshow(test_metrics[\"test_mask\"], cmap='gray', alpha=0.3)\nplt.title(\"Ground Truth Overlay\")\nplt.axis('off')\n\nplt.tight_layout()\nplt.show()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null}]}